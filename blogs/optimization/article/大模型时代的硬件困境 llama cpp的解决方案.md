# 大模型时代的硬件困境 —— llama.cpp的解决方案

## 项目地址
[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)

### 引言

随着人工智能技术的快速发展，我们已经迎来了大模型时代。在这个时代，模型的规模不断增大，复杂度不断提升，为各行各业带来了前所未有的机遇和挑战。然而，随着模型规模的扩大，推理效率问题逐渐凸显出来，成为制约大模型应用的瓶颈。因此，大模型时代的推理优化变得至关重要。

大模型的训练和推理是两个截然不同的过程。在训练阶段，我们追求的是更高的精度和更强的泛化能力，这需要大量的计算资源和时间。而在推理阶段，我们关注的是如何在保证精度的前提下，实现更快的响应速度和更低的计算成本。这就需要对推理过程进行针对性的优化。

为了应对这一挑战，我们引入了llama.cpp这一高效推理框架。llama.cpp是一个专为大规模模型设计的推理框架，它采用了一系列先进的优化技术，如模型压缩、硬件加速等，旨在提高推理速度和效率。同时，llama.cpp还具有良好的可扩展性和易用性，能够轻松应对不同场景和需求。截至撰写完成时，该项目在Github上已收货50.1K Stars。

通过引入llama.cpp这样的高效推理框架，我们可以更好地应对大模型时代的挑战，实现更快、更准、更省的计算目标。

### 主要特性

1. 纯C/C++构建，依赖极少
2. 多平台：Linux、Windows、同时也可部署在Docker上
3. Apple silicon 芯片一等公民支持，通过METAL的硬件调用；CPU、GPU、FPGA等芯片也可支持
4. X86架构的SEE、AVX、AVX2、AVX512 指令集支持
5. 混合精度推理，支持FP16/FP32，在保持精确度同时提升推理准确度
6. 量化工具绑定。量化是提升推理速度、降低硬件占用的一个重要手段。llama.cpp广泛支持模型量化至2bit、3bit、4bit、5bit、6bit、8bit。大大减少模型占用硬盘及显存容量。使在单张消费级显卡上完成大模型推理成为可能。量化后模型大小缩减示例如下表所示：

| Model | Measure | F16 | Q4_0 | Q4_1 | Q5_0 | Q5_1 | Q8_0 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 7B | 困惑度 | 5.9066 | 6.1565 | 6.0912 | 5.9862 | 5.9481 | 5.9070 |
| 7B | 模型大小 | 13.0G | 3.5G | 3.9G | 4.3G | 4.7G | 6.7G |
| 7B | ms/Token 4线程 | 127 | 55 | 54 | 76 | 83 | 72 |
| 7B | ms/Token 8线程 | 122 | 43 | 45 | 52 | 56 | 67 |
| 7B | 比特/参数量 | 16.0 | 4.5 | 5.0 | 5.5 | 6.0 | 8.5 |
| 13B | perplexity | 5.2543 | 5.3860 | 5.3608 | 5.2856 | 5.2706 | 5.2548 |
| 13B | file size | 25.0G | 6.8G | 7.6G | 8.3G | 9.1G | 13G |
| 13B | ms/Token 4线程 | - | 103 | 105 | 148 | 160 | 131 |
| 13B | ms/Token 8线程 | - | 73 | 82 | 98 | 105 | 128 |
| 13B | 比特/参数量 | 16.0 | 4.5 | 5.0 | 5.5 | 6.0 | 8.5 |

其中QX_Y表示做X精度的Y量化，当Y=0时参数近似值选择向下取整，当Y=1时参数近似向上取整。

量化后模型大小缩减较大，推理速度也大幅提升。在Q8情况下，困惑度

**2bit目前推理可能存在一定问题，解决方案持续更新中。**

7. 丰富的后端连接OpenCL、CUDA、METAL、OneAPI等等，
8. 边缘设备的支持，可较方便的将语言模型量化后部署到Jsten、Android及未来发布的AI PC等设备上，实现本地推理，数据保密的功能

## 易用特点

- 内存/显存规划合理，极大降低硬件占用，同时提供量化手段进一步降低硬件使用，中等参数量模型可在纯CPU设备上推理
- 广泛的中文模型支持：包括通义千问、易模型、百川1&2、LLaMa Chinese等，个人微调模型也可用过转换工具转换为框架可使用的GGML/GGUF格式使用
- 稳定性：需要context缓冲显存在运行初期便申请完成，不必担心由Python推理框架动态申请显存失败导致服务异常的情况。
- 多模态模型的支持，ShareGPT4V等，此处支持较少，可能后期需要再优化
- 与其它语言丰富的接口python、Go、Rust、Java、C#、Node.js等
- 灵活性： 可依据硬件情况自由连接加速组件，自由配置显存等参数，最大化利用计算资源

## 效果展示

Qwen-14B原始参数，未量化、硬件平台 Intel(R) Xeon(R) Gold 6254 CPU ，40线程

prompt：十个步骤建立一个简单网站基本测试

1. **Transformers推理**

CPU only：

内存 25.4G，1.49 Tokens/s 速度较慢

![Untitled](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E7%9A%84%E7%A1%AC%E4%BB%B6%E5%9B%B0%E5%A2%83%20llama%20cpp%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2001aa639047314b2ab39625e79a3eaed3/Untitled.png)

CPU+GPU：

![Untitled](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E7%9A%84%E7%A1%AC%E4%BB%B6%E5%9B%B0%E5%A2%83%20llama%20cpp%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2001aa639047314b2ab39625e79a3eaed3/Untitled%201.png)

![Untitled](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E7%9A%84%E7%A1%AC%E4%BB%B6%E5%9B%B0%E5%A2%83%20llama%20cpp%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2001aa639047314b2ab39625e79a3eaed3/Untitled%202.png)

显存占用 27.9G

1. **llama.cpp推理**

推理结果：

![Untitled](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E7%9A%84%E7%A1%AC%E4%BB%B6%E5%9B%B0%E5%A2%83%20llama%20cpp%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2001aa639047314b2ab39625e79a3eaed3/Untitled%203.png)

性能统计：

推理速度 3.67 Tokens/s 速度尚可接受

![Untitled](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E7%9A%84%E7%A1%AC%E4%BB%B6%E5%9B%B0%E5%A2%83%20llama%20cpp%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2001aa639047314b2ab39625e79a3eaed3/Untitled%204.png)

内存占用 26.6G RAM

链接GPU（NVIDIA A800 80G）后，显存占用与内存占用接近

![Untitled](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E7%9A%84%E7%A1%AC%E4%BB%B6%E5%9B%B0%E5%A2%83%20llama%20cpp%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2001aa639047314b2ab39625e79a3eaed3/Untitled%205.png)

推理速度达到70Tokens/s

![Untitled](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%E4%BB%A3%E7%9A%84%E7%A1%AC%E4%BB%B6%E5%9B%B0%E5%A2%83%20llama%20cpp%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2001aa639047314b2ab39625e79a3eaed3/Untitled%206.png)

## 效果评估

对于中等规模模型，llama.cpp实测速度相较基于Python的Transformers方法速度提升2倍以上，同时硬件占用未有提升。可在通用模型转化后进行推理部署。