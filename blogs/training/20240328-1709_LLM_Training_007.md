## 大模型-Firefly一站式大模型训练工具

Firefly 是一个开源的大模型训练项目，支持对主流的大模型进行微调，支持**全参训练、LoRA、QLoRA高效训练、预训练、SFT、DPO** 。

Firefly项目链接：[Firefly一站式大模型训练工具](https://github.com/yangjianxin1/Firefly/tree/master)

本文记录使用Firefly微调Qwen1.5-14B的全过程。

### 环境安装

首先将Firefly项目代码库clone到本地:

~~~bash
git clone https://github.com/yangjianxin1/Firefly.git
cd Firefly
~~~

创建python虚拟环境

~~~bash
conda create --name firefly python=3.10
conda activate firefly
~~~

安装python依赖包

~~~bash
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/
~~~

### 数据集准备

Firefly整理并开源了多个较高质量的指令数据集

| 数据集                                                       | 介绍                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [firefly-train-1.1M](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M) | 收集了23种常见的中文NLP任务的数据，并且构造了许多与中华文化相关的数据，如对联、作诗、文言文翻译、散文、金庸小说等，数据量为115万 |
| [moss-003-sft-data](https://huggingface.co/datasets/YeungNLP/moss-003-sft-data) | 由复旦大学MOSS团队开源的中英文多轮对话数据，包含100万+数据   |
| [ultrachat](https://huggingface.co/datasets/YeungNLP/ultrachat) | 由清华大学开源的英文多轮对话数据，包含140万+数据             |
| [WizardLM_evol_instruct_V2_143k](https://huggingface.co/datasets/YeungNLP/WizardLM_evol_instruct_V2_143k) | 由WizardLM项目开源的英文指令微调数据集，通过Evol-Instruct方法让指令进化，加强指令的复杂度，以提升模型对复杂指令的遵循能力。包含143k条数据。 |
| [school_math_0.25M](https://huggingface.co/datasets/YeungNLP/school_math_0.25M) | 由BELLE项目组开源的数学运算指令数据，包含25万条数据。        |
| [shareAI/CodeChat](https://huggingface.co/datasets/shareAI/CodeChat) | 主要包含逻辑推理、代码问答、代码生成相关语料样本。           |
| [shareAI/ShareGPT-Chinese-English-90k](https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k) | 中英文平行双语优质人机问答数据集，覆盖真实复杂场景下的用户提问。 |
| [ultrachat_200k](https://huggingface.co/datasets/YeungNLP/ultrachat_200k) | 由Zephyr项目开源的英文指令微调数据，在ultrachat数据基础上进行清洗 |
| [ultrafeedback_binarized](https://huggingface.co/datasets/YeungNLP/ultrafeedback_binarized) | 英文偏好数据集，可用于DPO训练                                |

### 配置训练参数

为了方便统一管理和更改Firefly的所有训练参数配置都存储在train_args目录下。主要参数说明如下：

| 主要参数                    | 参数说明                                                     |
| --------------------------- | ------------------------------------------------------------ |
| output_dir                  | 训练输出目录，存储checkpoint、tokenizer、tensorboard等       |
| model_name_or_path          | 预训练模型的本地目录，或者在huggingface上的模型名称          |
| train_file                  | 训练数据集路径，sft时，需要设置为文件，可以使用data/dummy_data.jsonl进行debug。pretrain时，需要设置为目录。脚本会自动扫描目录下的所有jsonl文件 |
| template_name               | 指令微调时，使用的模板名称。具体有哪些template_name，可参考component/template.py文件 |
| num_train_epochs            | 训练的轮次。如果数据量足够大，一般建议只训一个epoch          |
| tokenize_num_workers        | 预训练时，tokenize的线程数，默认为10                         |
| deepspeed                   | deepspeed的训练配置文件。全量参数训练时，将采用deepspeed     |
| train_mode                  | 训练模式，full、lora或qlora，默认为qlora                     |
| task_type                   | 任务类型，pretrain、sft或dpo，默认为sft                      |
| per_device_train_batch_size | 每张显卡的batch size                                         |
| gradient_accumulation_steps | 梯度累计步数。global batch=num_gpus * per_device_train_batch_size * gradient_accumulation_steps |
| gradient_checkpointing      | 如果显存捉襟见肘，可以开启。以时间换空间，模型不缓存激活状态，会进行两次forward计算，以节省显存 |
| learning_rate               | 学习率。全量参数微调的时候，建议小一些，1e-5或5e-6           |
| max_seq_length              | 训练时的最大长度。按照自己的设备进行设置，越长需要占用越多显存 |
| max_prompt_length           | 进行dpo时，prompt的最大长度                                  |
| logging_steps               | 每隔多少步统计一次train loss                                 |
| save_steps                  | 每隔多少步保存一个模型                                       |
| save_total_limit            | output_dir目录中最多保存多少个checkpoint，超出则会将最旧的删除 |
| lr_scheduler_type           | 学习率变化策略                                               |
| warmup_steps                | warm up步数。学习率经过多少步，增长到指定的数值              |
| optim                       | 优化器。如果是全量参数微调，建议使用adamw_hf                 |
| seed                        | 随机种子，用于复现实验结果                                   |
| fp16                        | 使用fp16混合精度。                                           |
| bf16                        | 使用bf16混合精度                                             |

当使用QLoRA训练的时候，需要设置以下参数：

| 参数         | 参数说明                                                     |
| ------------ | ------------------------------------------------------------ |
| lora_rank    | qlora矩阵的秩。一般设置为8、16、32、64等，在qlora论文中作者设为64。越大则参与训练的参数量越大，一般来说效果会更好，但需要更多显存 |
| lora_alpha   | qlora中的缩放参数。一般设为16、32即可                        |
| lora_dropout | lora权重的dropout rate                                       |

训练配置：

~~~json
{
    "output_dir": "output/firefly-qwen1.5-14b-sft-qlora",
    "model_name_or_path": "/your/model/path/to//Qwen/Qwen1.5-14B",
    "train_file": "./data/moss-003-sft-data.jsonl",
    "template_name": "qwen",
    "num_train_epochs": 1,
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 2,
    "learning_rate": 1e-4,
    "max_seq_length": 1024,
    "logging_steps": 300,
    "save_steps": 500,
    "save_total_limit": 1,
    "lr_scheduler_type": "constant_with_warmup",
    "warmup_steps": 3000,
    "lora_rank": 64,
    "lora_alpha": 16,
    "lora_dropout": 0.05,

    "gradient_checkpointing": true,
    "disable_tqdm": false,
    "optim": "paged_adamw_32bit",
    "seed": 42,
    "fp16": true,
    "report_to": "tensorboard",
    "dataloader_num_workers": 5,
    "save_strategy": "steps",
    "weight_decay": 0,
    "max_grad_norm": 0.3,
    "remove_unused_columns": false
}
~~~

### 启动训练

QLoRA指令微调Qwen1.5-14B

~~~bash
torchrun --nproc_per_node=1 train.py --train_args_file train_args/sft/qlora/qwen1.5-14b-sft-qlora.json
~~~

### 模型使用

#### 模型合并

使用LoRA或者QLoRA进行训练，Firefly仅保存adapter的权重和配置文件，需要将adapter权重与base model进行合并。修改合并脚本script/merge_lora.py代码中的save_path、adapter_name_or_path、model_name_or_path参数。

~~~python
import torch
"""
使用该脚本，将lora的权重合并大base model中
"""


def merge_lora_to_base_model():
    model_name_or_path = '/your/model/path/to//Qwen/Qwen1.5-14B'
    adapter_name_or_path = 'output/firefly-qwen1.5-14b-sft-qlora/checkpoint-20000'
    save_path = 'checkpoint/firefly-qwen1.5-14b-sft-qlora-merge'

    config = AutoConfig.from_pretrained(model_name_or_path)
    tokenizer = AutoTokenizer.from_pretrained(
        adapter_name_or_path,
        trust_remote_code=True,
        # llama不支持fast
        use_fast=False if config.model_type == 'llama' else True
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16,
        # device_map='auto',
        device_map={'': 'cpu'}
    )
    model = PeftModel.from_pretrained(model, adapter_name_or_path, device_map={'': 'cpu'})
    model = model.merge_and_unload()

    tokenizer.save_pretrained(save_path)
    model.save_pretrained(save_path)


if __name__ == '__main__':
    merge_lora_to_base_model()

~~~

运行脚本合并权重

~~~python
python script/merge_lora.py
~~~

#### 模型推理

合并权重之后，就可以是使用合并后的模型进行推理了。

修改 script/chat 目录下的 chat.py 代码中model_name_or_path，template_name参数

~~~python
from transformers import AutoTokenizer, AutoConfig, AddedToken
import torch
from loguru import logger
import copy

import sys
sys.path.append("../../")
from component.utils import ModelUtils
from component.template import template_dict


def build_prompt_chatglm3(tokenizer, query, history, system=None):
    history.append({"role": 'user', 'message': query})
    # system
    input_ids = tokenizer.get_prefix_tokens() + \
                [tokenizer.get_command(f"<|system|>")] + \
                tokenizer.encode(system, add_special_tokens=False)
    # convs
    for item in history:
        role, message = item['role'], item['message']
        if role == 'user':
            tokens = [tokenizer.get_command(f"<|user|>")] + \
                     tokenizer.encode(message, add_special_tokens=False) + \
                     [tokenizer.get_command(f"<|assistant|>")]
        else:
            tokens = tokenizer.encode(message, add_special_tokens=False) + [tokenizer.eos_token_id]
        input_ids += tokens

    return input_ids


def build_prompt(tokenizer, template, query, history, system=None):
    template_name = template.template_name
    system_format = template.system_format
    user_format = template.user_format
    assistant_format = template.assistant_format
    system = system if system is not None else template.system

    if template_name == 'chatglm2':
        prompt = tokenizer.build_prompt(query, history)
        input_ids = tokenizer.encode(prompt)
    elif template_name == 'chatglm3':
        input_ids = build_prompt_chatglm3(tokenizer, query, history, system)
    else:
        history.append({"role": 'user', 'message': query})
        input_ids = []

        # setting system information
        if system_format is not None:
            # system信息不为空
            if system is not None:
                system_text = system_format.format(content=system)
                input_ids = tokenizer.encode(system_text, add_special_tokens=False)
        # concat conversation
        for item in history:
            role, message = item['role'], item['message']
            if role == 'user':
                message = user_format.format(content=message, stop_token=tokenizer.eos_token)
            else:
                message = assistant_format.format(content=message, stop_token=tokenizer.eos_token)
            tokens = tokenizer.encode(message, add_special_tokens=False)
            input_ids += tokens
    input_ids = torch.tensor([input_ids], dtype=torch.long)

    return input_ids


def load_tokenizer(model_name_or_path):
    # config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)
    # 加载tokenzier
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path,
        trust_remote_code=True,
        use_fast=False
        # llama不支持fast
        # use_fast=False if config.model_type == 'llama' else True
    )

    if tokenizer.__class__.__name__ == 'QWenTokenizer':
        tokenizer.pad_token_id = tokenizer.eod_id
        tokenizer.bos_token_id = tokenizer.eod_id
        tokenizer.eos_token_id = tokenizer.eod_id
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # assert tokenizer.pad_token_id is not None, "pad_token_id should not be None"
    return tokenizer


def main():
    # 使用合并后的模型进行推理
    model_name_or_path = 'checkpoint/firefly-qwen1.5-14b-sft-qlora-merge'
    template_name = 'qwen'
    adapter_name_or_path = None

    template = template_dict[template_name]
    # 是否使用4bit进行推理，能够节省很多显存，但效果可能会有一定的下降
    load_in_4bit = False
    # 生成超参配置
    max_new_tokens = 500
    top_p = 0.9
    temperature = 0.35
    repetition_penalty = 1.0

    # 加载模型
    logger.info(f'Loading model from: {model_name_or_path}')
    logger.info(f'adapter_name_or_path: {adapter_name_or_path}')
    model = ModelUtils.load_model(
        model_name_or_path,
        load_in_4bit=load_in_4bit,
        adapter_name_or_path=adapter_name_or_path
    ).eval()
    tokenizer = load_tokenizer(model_name_or_path if adapter_name_or_path is None else adapter_name_or_path)
    if template_name == 'chatglm2':
        stop_token_id = tokenizer.eos_token_id
    elif template_name == 'chatglm3':
        stop_token_id = [tokenizer.eos_token_id, tokenizer.get_command("<|user|>"), tokenizer.get_command("<|observation|>")]
    else:
        if template.stop_word is None:
            template.stop_word = tokenizer.eos_token
        stop_token_id = tokenizer.encode(template.stop_word, add_special_tokens=False)
        assert len(stop_token_id) == 1
        stop_token_id = stop_token_id[0]

    history = []

    query = input('User：')
    while True:
        query = query.strip()
        input_ids = build_prompt(tokenizer, template, query, copy.deepcopy(history), system=None).to(model.device)
        outputs = model.generate(
            input_ids=input_ids, max_new_tokens=max_new_tokens, do_sample=True,
            top_p=top_p, temperature=temperature, repetition_penalty=repetition_penalty,
            eos_token_id=stop_token_id
        )
        outputs = outputs.tolist()[0][len(input_ids[0]):]
        response = tokenizer.decode(outputs)
        response = response.strip().replace(template.stop_word, "").strip()
        # update history
        history.append({"role": 'user', 'message': query})
        history.append({"role": 'assistant', 'message': response})

        print("Firefly：{}".format(response))
        query = input('User：')


if __name__ == '__main__':
    main()
~~~

运行脚本进行推理

~~~python
cd script/chat
python chat.py
~~~

### 总结

本文使用Firefly项目微调Qwen1.5-14B模型，希望对大家有所帮助。

### 参考文献

- Firefly项目代码 (https://github.com/yangjianxin1/Firefly/tree/master)
