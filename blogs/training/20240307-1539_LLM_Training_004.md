## 大模型 - 开源大模型 OLMo

艾伦人工智能研究所（AI2）联合多个顶尖学术机构发布了史上首个100%开源的大模型“OLMo”！它的英文全称就叫Open Language Model。

### OLMo的独特之处是**完全开源**：

- 完整的训练数据，包括生成训练数据的代码

- 完整的训练和评估代码

- 中间模型检查点，每个基础模型有500多个检查点，来自训练过程中每1000个步骤

- 训练日志

### OLMo目前开源的模型主要有三个规模

| Size | Training Tokens | Training Config | Layers | Hidden Size | Attention Heads |
|------|-----------------|-----------------|---------|-------------|-----------------|
| [1B](https://huggingface.co/allenai/OLMo-1B)   | 2T | [configs/official/OLMo-1B.yaml](https://github.com/allenai/OLMo/blob/main/configs/official/OLMo-1B.yaml) | 16 | 2048 | 16 |
| [7B](https://huggingface.co/allenai/OLMo-7B)   | 3T | [configs/official/OLMo-7B.yaml](https://github.com/allenai/OLMo/blob/main/configs/official/OLMo-7B.yaml) | 32 | 4096 | 32 |
| 65B*                                           |  |  | 80 | 8192 | 64 |

其中65B的模型还在训练中，目前开源的最大模型是OLMo 7B

### Dolma简介

Dolma主要由两部分组成：

- Dolma Dateset：一个包含3万亿tokens的数据集，该数据集包含网页内容、学术出版物、代码、书籍、百科全书等，该数据大小约5.4TB。

- Dolma Toolkit：一个用于整理语言建模数据集的高性能工具包

#### Dolma Dateset

##### 数据集统计结果 v1.6(发布于2024-01-31)

| 来源 | 类型 | 大小（GB）| 文档数量（百万） | Llama tokens(十亿)
|-----|------|---------|---------------|---------------|
| Common Crawl | 网页 | 9,022 | 3,370 |2,281|
| The Stack | 代码 | 1,043 | 210 |411|
| C4 | 网页 | 790 | 364 |198|
| Reddit | 社媒 | 339 | 377 |89|
| PeS2o | 学术 | 268 | 38.8 |70|
| Project Gutenberg | 书籍 | 20.4 | 0.056 |6.0|
| Wikipedia, Wikibooks | 百科 | 16.2 | 6.2 |4.3|
| 总计 | | 11,519 | 4,367 |3,059|

#### Dolma Toolkit

##### 特点
- 高性能：由于内置并行性，可以同时处理数十亿个文档。
- 可移植性：适用于单机、集群或云环境。
- 快速去重：使用 Rust Bloom 过滤器快速进行文档重复数据删除。
- 可扩展：支持自定义标记器和AWS S3兼容位置。
- 内置标记器：包括通常用于管理数据集的现成标记器，

##### 安装
~~~bash
pip install dolma
~~~

#### OLMo

##### 安装

~~~bash
git clone https://github.com/allenai/OLMo.git

pip install ai2-olmo
~~~

##### 推理

```python
from transformers import pipeline
olmo_pipe = pipeline("text-generation", model="allenai/OLMo-7B") # 这里可以直接指定自己的目录
print(olmo_pipe("Language modeling is"))

## 输出
## Language modeling is a process of training a machine learning model to learn from data...
```

##### 量化

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

olmo = AutoModelForCausalLM.from_pretrained("allenai/OLMo-7B", torch_dtype=torch.float16, load_in_8bit=True)
```

##### 训练

~~~bash
torchrun --nproc_per_node=8 scripts/train.py configs/official/OLMo-1B.yaml
~~~

##### 微调

```
torchrun --nproc_per_node=8 scripts/train.py {path_to_train_config} \
    --data.paths=[{path_to_data}/input_ids.npy] \
    --data.label_mask_paths=[{path_to_data}/label_mask.npy] \
    --load_path={path_to_checkpoint} \
    --reset_trainer_state
```

##### 评估

OMLo还提供了一个用于评估开源模型的仓库OLMo-Eval，使用此管道，可以评估 t 个任务集上的 m 个模型，其中每个任务集由一个或多个单独的任务组成。使用task_sets 允许您计算多个任务的聚合指标。可选集成可用于报告。

### 小结

虽然OLMo在效果上并没有那么惊艳，但是为AI研究提供了大模型宝贵的资源，有助于降低研究和开发的门槛，推动AI技术的创新和发展。

OLMo的发布，标志着AI开源模型进入了一个新的时代。随着越来越多的研究机构和企业加入到开源的行列，相信未来的AI技术将更加开放、透明和创新。

### 相关资源

- https://allenai.org/olmo 官网

- https://github.com/allenai/OLMo OLMo代码

- https://huggingface.co/datasets/allenai/dolma 训练数据

- https://huggingface.co/allenai/OLMo-7B 模型权重数据

- https://github.com/allenai/dolma Dolma代码

- https://github.com/allenai/ai2-olmo-eval OLMo评估代码
