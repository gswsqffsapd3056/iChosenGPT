## 多模态大模型微调实现

下面将LLM微调成图文 多模态大模型为例，来实现转换过程。



### LLM大模型参数冻结

在前面有聊过将LLM拓展为多模态模型，在现有的工作中，通常将预训练模型进行”冻结“，**这里的”冻结“的意思是将预训练模型的权重信息只进行前向传播，不参与模型的反向传播，不进行可训练权重的更新。**

下面以文本大模型Qwen-14B为例，进行权重的冻结，具体实现如下：

~~~python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-14B", device_map="auto", trust_remote_code=True).eval()
for p in model.parameters():
    p.requires_grad = False
~~~

> 该方法是从hugging face官网上下载[Qwen-14B](https://huggingface.co/Qwen/Qwen-14B)的权重文件，你可以提前将该模型的相关权重下载到本地，将上述代码中的`"Qwen/Qwen-14B"`换成你本地路径即可



### 图像编码器参数冻结

上面只是针对大模型上的编码和解码过程，要实现图文多模态，需要加入对图像的编码和解码操作，这里你可以选择经典的ResNet系列模型，也可以使用ViT系列模型，当然也可以使用自己的图像编码模型，这个需要看自己的需求。（个人还是非常喜欢使用ViT系列的模型，该系列模型是通过将图像切patch的方式作为token输入到transformer中。）

下面以自己的图像编码器为例，实现模型参数冻结:

~~~python
#假设图像编码模型为 visionModel，训练权重为checkpoint_1112.pth
import torch
import torch.nn as nn
#简单的模型实现，现实非常复杂，可能需要进行attention等方式。
class visionModel(nn.Module):
    def __init__(self,):
        super(visionModel,self).__init__()
        self.cnv1 = nn.Conv2d(3,16,kernel_size=3, stried=1, padding=1)
        self.relu = nn.ReLu()
        self.fc = nn.Linear(1024)
    def forward(self,x):
        out = self.conv1(x)
        out = self.relu(out)
        out = self.fc(out)
        return out
freeze_vit = True
vis_model = visionModel()
static_dict = torch.load("checkpoint_1112.pth",map_location="cpu")
vis_model.load_state_dict(static_dict)
if freeze_vit:
    for name,param in vis_model.named_parameters():
        param.requires_grad = False
vis_model = vis_model.eval()
~~~

> 如果你使用现有的图像编码模型，使用方式和 `LLM大模型参数冻结`实现方式一样



### 文本编码器

上面介绍了图像编码器的使用与权重加载，在文本编码器中你也可以使用与之前的相同操作，使用`bert`作为文本编码器，然后也进行权重的冻结操作，当然你也可以让该部分参与到模型的训练中。

- 如果将文本编码器进行冻结的话，需要在图像编码器和文本编码器之后分别加入Linear层，将图像，文本的文本空间进行对齐，比如`LLaVA和vary`模型
- 如果将文本编码器不进行冻结，需要在文本编码中加入某些网络层，或者权重矩阵参与运算，让该部分参与到训练中，比如`Q-form`系列模型



### 权重类型转换

在进行模型训练过程中，需要保证数据流中的数据类型是一致的，这时候需要进行权重类型转化，将LLM和vision权重类型进行权重转化，如果将权重类型设置成int16，int8的推理过程也会提升速度。

~~~python
def convert_weights_to_fp16(model):
#将适用的模型参数转换为fp16
    def _convert_weights_to_fp16(l):
        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):
            l.weight.data = l.weight.data.half()
            if l.bias is not None:
                l.bias.data = l.bias.data.half()
    model.apply(_convert_weights_to_fp16)
~~~



### 图像和文本embedding拼接送入到LLM中

上述过程已经得到图像嵌入表示和文本嵌入表示，只需要将这两部分进行concat操作，送入到模型的embedding层即可（这里不是送入token id）

当然在concat之后送入到大模型之前，需要将拼接之后的嵌入表示进行转换（比如Linear等方式），该过程依然需要进行训练。



### 总结

1. 过程1：将图像，文本编码器生成的低置嵌入进行合并，需要首先进行训练，得到权重文件。
2. 过程2：将1中训练好的模型与大模型进行融合，该过程同样也需要进行训练。