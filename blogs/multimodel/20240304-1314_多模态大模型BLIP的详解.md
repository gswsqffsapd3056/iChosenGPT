# 多模态大模型BLIP的详解

之前有简单介绍过多模态大模型的发展（比如：[目前的相关工作](20240202-1945_从文本大模型到多模态大模型.md)，以及[逻辑实现](20240205-1952_多模态大模型微调逻辑实现.md)），这次将以**BLIP为例**，将详细的介绍一下其中的实现方式。

## 1.LLM做多模态任务——ViT

在SORA的解读报告中，我们有针对SORA在图像编码器(DiT)与[其他图像编码（比如ViT）](20240226-1123_多模态大模型中图像编码器DiT与ViT.md)有介绍，这里不再进行过多的描述，这里简单的总结一下常用的视觉大模型。

| 预训练范式 | 视觉大模型 |
| :--------: | :--------: |
| 有监督学习 |   ViT等    |
| 多模态学习 |   CLIP等   |
| 自监督学习 |   MAE等    |

图像和自然语言具有不同的结构：图像由低维像素构成的矩阵表示，而自然语言则由高维的单词向量构成的数组表示。因此，相比于自然语言，利用大型视觉模型（LVM）对单个"图像的词向量"进行嵌入是困难的，LVM更适合提取整个图像矩阵的特征（可以将其类比为BERT在特殊的词元位置处进行整个句子语义嵌入）。

后期也将陆续的介绍其他的图像编码器在多模态大模型上的应用。

## 2. LLM做多模态任务——BERT

BERT是一个相对古早但是非常有效的Transformer类模型，关于BERT的介绍和源码解读文章也非常多，其源码开源在Huggingface的transformer库中。

### 2.1 Embedding

这个类是对分词之后的token做嵌入，包括word和position两种类型的嵌入，在某些论文里可能会增加其余类型的嵌入，但是逻辑是一样的。

~~~python
import torch
import numpy
import torch.nn as nn 

class BertEmbeddings(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.register_buffer('position_ids', torch.arange(config.max_position_embeddings).expand((1, -1)))
        self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
        self.config = config

    def forward(self, input_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0):
        if input_ids is not None:
            input_shape = input_ids.size()
        else:
            input_shape = inputs_embeds.size()[:-1]  
        seq_length = input_shape[1]

        if position_ids is not None:
            position_ids = self.position_ids[:,past_key_values_length:past_key_values_length + seq_length]

        if inputs_embeds is None:
            inputs_embeds = self.word_embeddings(input_ids)

        embeddings = inputs_embeds

        if self.position_embedding_type == "absolute":
            position_embeddings = self.position_embeddings(position_ids)
            embeddings += position_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
~~~

### 2.2 自注意力类

~~~python
class BertSelfAttention(nn.Module):

    def __init__(self, config, is_cross_attention):
        super().__init__()
        self.config = config
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, "embedding_size"):
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads)
            )
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)

        if is_cross_attention:
            self.key = nn.Linear(config.encoder_width, self.all_head_size)
            self.value = nn.Linear(config.encoder_width, self.all_head_size)e
        else:
            self.key = nn.Linear(config.hidden_size, self.all_head_size)
            self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")

        if self.position_embedding_type == "relative_key" or self.position_embedding_type == "relative_key_query":
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)
        self.save_attention = False   

    def save_attn_gradients(self, attn_gradients):
        self.attn_gradients = attn_gradients
        
    def get_attn_gradients(self):
        return self.attn_gradients
    
    def save_attention_map(self, attention_map):
        self.attention_map = attention_map
        
    def get_attention_map(self):
        return self.attention_map

    # 调整注意力的格式
    def transpose_for_scores(self, x):
        x = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(
        self,
        hidden_states, # 输入的隐状态变量，一般为text的编码
        attention_mask=None, # 隐状态变量对应的attention mask，表示哪些token需要参与到attention计算中
        head_mask=None, # 使bert的某些头的运算无效化
        encoder_hidden_states=None, # cross-attention时候使用，代表另一个编码器的输入，通常为视觉编码器ViT
        encoder_attention_mask=None, # 另一个编码器输入对应的mask
        past_key_value=None, # 上面介绍过，一般在解码器推理时候用到
        output_attentions=False, # 最后是否输出attention
    ):
        # hidden_states维度为(batch_size, seq_len, hidden_size)，经过映射后得到的query vector维度为(batch_size, seq_len, all_head_size)
        mixed_query_layer = self.query(hidden_states)

        # If this is instantiated as a cross-attention module, the keys
        # and values come from an encoder; the attention mask needs to be
        # such that the encoder's padding tokens are not attended to.
        is_cross_attention = encoder_hidden_states is not None # 有输入encoder_hidden_states才是cross-attention

        if is_cross_attention:
            # 如果是cross attention操作，则query来自hidden_states，而key和value则来自encoder_hidden_states
            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))
            attention_mask = encoder_attention_mask
        elif past_key_value is not None:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))
            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)
        else:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))

        
        query_layer = self.transpose_for_scores(mixed_query_layer)
        past_key_value = (key_layer, value_layer) # 此时用最新的key和value来更新past_key_value，从而重复进行解码

        # Take the dot product between "query" and "key" to get the raw attention scores.
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # query和key运算获得最初的attention score, 维度为(batch_size, num_attention_heads, seq_len, len_key = seq_len + len_past_key_values)
        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # scale，保证均值和方差稳定

        if attention_mask is not None:
            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
            attention_scores = attention_scores + attention_mask
        attention_probs = nn.Softmax(dim=-1)(attention_scores)

        if is_cross_attention and self.save_attention: # 表明是否需要输出attention score
            self.save_attention_map(attention_probs)
            attention_probs.register_hook(self.save_attn_gradients)   

        # This is actually dropping out entire tokens to attend to, which might
        # seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs_dropped = self.dropout(attention_probs) # 这里采取dropout确实有点奇怪，理论上应该是在获取到最后的加
        if head_mask is not None:
            attention_probs_dropped = attention_probs_dropped * head_mask
        context_layer = torch.matmul(attention_probs_dropped, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)
        outputs = outputs + (past_key_value,)
        return outputs

class BertAttention(nn.Module):
'''
BertAttention类就是整合2和3的BertSelfAttention类以及BertSelfOutput类，最终构建一个完整的Attention模块
'''
    def __init__(self, config, is_cross_attention=False):
        super().__init__()
        self.self = BertSelfAttention(config, is_cross_attention)
        self.output = BertSelfOutput(config)

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
    ):
        self_outputs = self.self(
            hidden_states,
            attention_mask,
            head_mask,
            encoder_hidden_states,
            encoder_attention_mask,
            past_key_value,
            output_attentions,
        )
        # 注意这里的最终输出是hidden_states + Attention(hidden_states)，遵循残差连接设计
        attention_output = self.output(self_outputs[0], hidden_states)
        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them
        return outputs
~~~

> transpose_for_scores函数：这个函数并不复杂，主要是用于调整key, query, value的维度，其输入的x的维度为(batch_size, seq_len, all_head_dim)，经过reshape之后的new_x_shape的维度为(batch_size, seq_len, num_head, head_dim)，相当于把all_head_dim的维度拆分到每个头上，例如一般all_head_dim为768，num_head一般为12，则每个头对应的特征维度为768 / 12 = 64，然后每个头可以并行运算，互不干扰，这也是为什么Transformer模型具有很好的并行性的原因，多头注意力机制非常重要。

### 2.3 Layer类

~~~python
class BertLayer(nn.Module):

    def __init__(self, config, layer_num):
        super().__init__()
        self.config = config
        self.chunk_size_feed_forward = config.chunk_size_feed_forward 
        self.seq_len_dim = 1 
        self.attention = BertAttention(config)
        self.layer_num = layer_num 
        if self.config.add_cross_attention: 
            self.crossattention = BertAttention(config, is_cross_attention=self.config.add_cross_attention)
        self.intermediate = BertIntermediate(config)
        self.output = BertOutput(config)

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
        mode=None,
    ):
        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2
        # 将Bert作为decoder时，如果有past_key_value则直接使用，加速解码
        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None

        # 先进行self attention运算
        self_attention_outputs = self.attention(
            hidden_states,
            attention_mask,
            head_mask,
            output_attentions=output_attentions,
            past_key_value=self_attn_past_key_value,
        )
        # self attention运算的输出self_attention_outputs是一个元组，其中第一个元素是hidden_states经过attention运算后的输出
        attention_output = self_attention_outputs[0]
        outputs = self_attention_outputs[1:-1]
        present_key_value = self_attention_outputs[-1]
        
        # 如果需要进行cross-attention运算，此时Bert Encoder就是"multimodal_encoder"，此时就必须要有encoder_hidden_states这个输入，该输入通常是ViT编码得到的图像patch序列
        if mode=='multimodal':
            assert encoder_hidden_states is not None, "encoder_hidden_states must be given for cross-attention layers"
            # 很容易发现，相比于self.attention，self.crossattention的输入多了encoder_hidden_states
            cross_attention_outputs = self.crossattention(
                attention_output,
                attention_mask,
                head_mask,
                encoder_hidden_states,
                encoder_attention_mask,
                output_attentions=output_attentions,
            )
            attention_output = cross_attention_outputs[0]
            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights                   
        layer_output = apply_chunking_to_forward(
            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output
        )
        # layer_output就是在一层layer中经过Attention和FFN运算之后的最终输出
        outputs = (layer_output,) + outputs
        outputs = outputs + (present_key_value,)

        return outputs

    def feed_forward_chunk(self, attention_output):
        # 该函数就是将Attention运算得到的attention_output进行FFN运算，并做残差连接
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output
~~~

## 3. LLM做多模态任务——BLIP

BLIP做ITM和ITC的核心逻辑不复杂，但是在源码中作者为了有效去噪，采用了动量的思想，同时在做ITM二分类时除了常规的正样本对，作者还利用hard-negative-mining的策略，构造了一批负样本对，最终的损失就是ITM + ITC，这部分内容也不涉及解码，因此相对容易理解。

~~~python
class BLIP_Retrieval(nn.Module):
    def __init__(self,                 
                 med_config = 'configs/med_config.json',  
                 image_size = 384,
                 vit = 'base',
                 vit_grad_ckpt = False,
                 vit_ckpt_layer = 0,                      
                 embed_dim = 256,     
                 queue_size = 57600,
                 momentum = 0.995,
                 negative_all_rank = False,
                 ):
        """
        Args:
            med_config (str): path for the mixture of encoder-decoder model's configuration file
            image_size (int): input image size
            vit (str): model size of vision transformer
        """               
        super().__init__()
        # 视觉编码器ViT
        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)
        # 文本/多模态编码器 BERT
        self.tokenizer = init_tokenizer()   
        med_config = BertConfig.from_json_file(med_config)
        med_config.encoder_width = vision_width
        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)          

        text_width = self.text_encoder.config.hidden_size
        # 做ITC
        self.vision_proj = nn.Linear(vision_width, embed_dim)
        self.text_proj = nn.Linear(text_width, embed_dim)

        # ITM head
        self.itm_head = nn.Linear(text_width, 2) 
        
        # create momentum encoders  
        self.visual_encoder_m, vision_width = create_vit(vit,image_size)              
        self.vision_proj_m = nn.Linear(vision_width, embed_dim)
        self.text_encoder_m = BertModel(config=med_config, add_pooling_layer=False)    
        self.text_proj_m = nn.Linear(text_width, embed_dim)
        
        self.model_pairs = [[self.visual_encoder,self.visual_encoder_m],
                            [self.vision_proj,self.vision_proj_m],
                            [self.text_encoder,self.text_encoder_m],
                            [self.text_proj,self.text_proj_m],
                           ]       
        self.copy_params()

        # create the queue
        self.register_buffer("image_queue", torch.randn(embed_dim, queue_size))
        self.register_buffer("text_queue", torch.randn(embed_dim, queue_size))
        self.register_buffer("idx_queue", torch.full((1,queue_size),-100))
        self.register_buffer("ptr_queue", torch.zeros(1, dtype=torch.long))  

        self.image_queue = nn.functional.normalize(self.image_queue, dim=0)
        self.text_queue = nn.functional.normalize(self.text_queue, dim=0)
        
        self.queue_size = queue_size
        self.momentum = momentum
        self.temp = nn.Parameter(0.07*torch.ones([]))   
        self.negative_all_rank = negative_all_rank

    def forward(self, image, caption, alpha, idx):
        # 输入的image和caption在一个批次中是pair的
        with torch.no_grad():
            self.temp.clamp_(0.001,0.5)
        # 视觉侧图像特征提取，标准的Vision Transformer操作
        image_embeds = self.visual_encoder(image) 
        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)        
        image_feat = F.normalize(self.vision_proj(image_embeds[:,0,:]),dim=-1)    

        # 文本侧特征提取，注意此时text_encoder没有输入image_embeds，因此仅仅是单纯的文本self-attention编码
        text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=35, 
                              return_tensors="pt").to(image.device) 
        
        text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      
                                        return_dict = True, mode = 'text')            
        text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:,0,:]),dim=-1)        
        
        # 这一段就是传统的对比学习实现，使用动量方法是为了去噪，消除假的正样本对
        idx = idx.view(-1,1)
        idx_all = torch.cat([idx.t(), self.idx_queue.clone().detach()],dim=1)  
        pos_idx = torch.eq(idx, idx_all).float()       
        sim_targets = pos_idx / pos_idx.sum(1,keepdim=True)   
        
        # get momentum features
        with torch.no_grad():
            self._momentum_update()
            image_embeds_m = self.visual_encoder_m(image) 
            image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:,0,:]),dim=-1)  
            image_feat_m_all = torch.cat([image_feat_m.t(),self.image_queue.clone().detach()],dim=1)                   
            
            text_output_m = self.text_encoder_m(text.input_ids, attention_mask = text.attention_mask,                      
                                                return_dict = True, mode = 'text')    
            text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:,0,:]),dim=-1) 
            text_feat_m_all = torch.cat([text_feat_m.t(),self.text_queue.clone().detach()],dim=1)

            sim_i2t_m = image_feat_m @ text_feat_m_all / self.temp  
            sim_t2i_m = text_feat_m @ image_feat_m_all / self.temp   

            sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets
            sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets        

        sim_i2t = image_feat @ text_feat_m_all / self.temp 
        sim_t2i = text_feat @ image_feat_m_all / self.temp 
                             
        loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1)*sim_i2t_targets,dim=1).mean()
        loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1)*sim_t2i_targets,dim=1).mean() 

        loss_ita = (loss_i2t+loss_t2i)/2
        
        idxs = concat_all_gather(idx)
        self._dequeue_and_enqueue(image_feat_m, text_feat_m, idxs)        

        
        
        encoder_input_ids = text.input_ids.clone()
        encoder_input_ids[:,0] = self.tokenizer.enc_token_id # 通过在caption前设置不同的的标记来标识不同的任务

        # forward the positve image-text pair，此时encoder_hidden_states参数不再是None，表明此时是做Cross-attention运算，最终得到的是融合后的多模态特征
        bs = image.size(0)
        # 注意此时输入至encoder的image-text pair全是正样本对，因此它们融合得到的多模态特征做二分类时应该为1，代表正例
        output_pos = self.text_encoder(encoder_input_ids,
                                       attention_mask = text.attention_mask,
                                       encoder_hidden_states = image_embeds,
                                       encoder_attention_mask = image_atts,      
                                       return_dict = True,
                                      )  
        
        
        if self.negative_all_rank:    
            # compute sample similarity
            with torch.no_grad():                
                mask = torch.eq(idx, idxs.t())

                image_feat_world = concat_all_gather(image_feat)
                text_feat_world = concat_all_gather(text_feat)

                sim_i2t = image_feat @ text_feat_world.t() / self.temp 
                sim_t2i = text_feat @ image_feat_world.t() / self.temp 

                weights_i2t = F.softmax(sim_i2t,dim=1)
                weights_i2t.masked_fill_(mask, 0)            

                weights_t2i = F.softmax(sim_t2i,dim=1)
                weights_t2i.masked_fill_(mask, 0)     

            image_embeds_world = all_gather_with_grad(image_embeds) 

            # select a negative image (from all ranks) for each text
            image_embeds_neg = []    
            for b in range(bs):
                neg_idx = torch.multinomial(weights_t2i[b], 1).item()
                image_embeds_neg.append(image_embeds_world[neg_idx])
            image_embeds_neg = torch.stack(image_embeds_neg,dim=0)   

            # select a negative text (from all ranks) for each image
            input_ids_world = concat_all_gather(encoder_input_ids)
            att_mask_world = concat_all_gather(text.attention_mask)        

            text_ids_neg = []
            text_atts_neg = []
            for b in range(bs):
                neg_idx = torch.multinomial(weights_i2t[b], 1).item()
                text_ids_neg.append(input_ids_world[neg_idx])
                text_atts_neg.append(att_mask_world[neg_idx])
                
        else:
            with torch.no_grad():                
                mask = torch.eq(idx, idx.t())
                
                sim_i2t = image_feat @ text_feat.t() / self.temp 
                sim_t2i = text_feat @ image_feat.t() / self.temp 

                weights_i2t = F.softmax(sim_i2t,dim=1)
                weights_i2t.masked_fill_(mask, 0)            

                weights_t2i = F.softmax(sim_t2i,dim=1)
                weights_t2i.masked_fill_(mask, 0)     

            # select a negative image (from same rank) for each text
            image_embeds_neg = []    
            for b in range(bs):
                neg_idx = torch.multinomial(weights_t2i[b], 1).item()
                image_embeds_neg.append(image_embeds[neg_idx])
            image_embeds_neg = torch.stack(image_embeds_neg,dim=0)   

            # select a negative text (from same rank) for each image    
            text_ids_neg = []
            text_atts_neg = []
            for b in range(bs):
                neg_idx = torch.multinomial(weights_i2t[b], 1).item()
                text_ids_neg.append(encoder_input_ids[neg_idx])
                text_atts_neg.append(text.attention_mask[neg_idx])            
            
        text_ids_neg = torch.stack(text_ids_neg,dim=0)   
        text_atts_neg = torch.stack(text_atts_neg,dim=0)      

        # 上面一段代码实际上是采用了难样本挖掘的思路为每一个text找到了一些反例image，以及为每一个image找到了一些反例text
        text_ids_all = torch.cat([encoder_input_ids, text_ids_neg],dim=0)     
        text_atts_all = torch.cat([text.attention_mask, text_atts_neg],dim=0)     

        image_embeds_all = torch.cat([image_embeds_neg,image_embeds],dim=0)
        image_atts_all = torch.cat([image_atts,image_atts],dim=0)

        # 此时由于输入到encoder的都是负样本对，因此融合特征做二分类时应该分类结果为0
        output_neg = self.text_encoder(text_ids_all,
                                       attention_mask = text_atts_all,
                                       encoder_hidden_states = image_embeds_all,
                                       encoder_attention_mask = image_atts_all,      
                                       return_dict = True,
                                      )                         
          

        vl_embeddings = torch.cat([output_pos.last_hidden_state[:,0,:], output_neg.last_hidden_state[:,0,:]],dim=0)
        vl_output = self.itm_head(vl_embeddings)            

        # 正样本对标签为1，负样本对标签为0
        itm_labels = torch.cat([torch.ones(bs,dtype=torch.long),torch.zeros(2*bs,dtype=torch.long)],
                               dim=0).to(image.device)
        loss_itm = F.cross_entropy(vl_output, itm_labels)

        return loss_ita, loss_itm
~~~

## 其他扩展 zero-shot

**Zero-shot学习，是指在训练阶段不存在与测试阶段完全相同的类别**，但是模型可以使用训练过的知识来推广到测试集中的新类别上。这种能力被称为“零样本”学习，因为模型在训练时从未见过测试集中的新类别。

在CLIP、gpt-2和gpt-3这种大模型中， zero-shot含义稍微有一点不一样，

在CLIP模型中，**Zero-shot表示模型可以在没有针对特定任务进行微调的情况下，对新数据进行分类预测**。也就是说，CLIP模型可以通过使用语言和图像之间的对比学习来对新数据进行分类，**而无需针对特定任务和类别进行微调。**

Zero-shot是CLIP模型的一个重要特性，使得模型可以预测新的、未见过的类别和任务，从而具有更好的泛化性能，使得模型更加灵活和通用。
