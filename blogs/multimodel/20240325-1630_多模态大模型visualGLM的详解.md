# 多模态大模型VisualGLM的详解

## visualGLM

清华大学开源的预训练大语言模型 chatglm-6B 已在国内被开发者熟知和广泛使用。据其官网介绍该方案是联合了语言模型 chatglm-6b 和 BLIP2-Qformer 构建的视觉模型。

>  开源项目地址：[https://github.com/THUDM/VisualGLM-6B](https://link.zhihu.com/?target=https%3A//github.com/THUDM/VisualGLM-6B)
>
> VisualGLM 体验demo地址: [https://huggingface.co/spaces/lykeven/visualglm-6b](https://huggingface.co/spaces/lykeven/visualglm-6b)

### 1. 多模态预训练背景

以BERT/GPT为代表的自回归模型展现出强大的理解生成能力。同样的，如何将这种成功“复制”到视觉/跨模态领域？其中以DALL-E、CogView、BEiT等为代表的模型思路为

（1）将图像当成一种语言进行预训练；

（2）离散化token。

以CogView为例，输入token既包含了文本token，也包含了Image token，所以Transformer既建模了文本也建模了图像，并且建模了文本到图像这种跨模态的过程。

但是该思路存在问题：会损失底层信息；token利用效率低，比如需要>1000tokens来描述一张256分辨率的图，此外训练过程也是较难稳定的。

实际上，人类只对少量的视觉语义信息感兴趣，那该如何提升效率并充分利用预研模型呢？BLIP2提供的思路如下：将图像特征对齐到预训练语言模型。

优点：充分得利用了语言模型，并且无缝结合原有的多轮对话能力，

缺点：但是提取图像语义特征会损失底层信息。

### 2. 本地部署

从项目代码路径下载代码到本地，

~~~python
https://github.com/THUDM/VisualGLM-6B
~~~

然后执行以下安装依赖。

~~~python
pip install requirements.txt
~~~

从以下路径下载模型文件放于VisualGLM-6B文件夹下，权重被拆分成5个文件，一共约有22G大小。

~~~PYTHON
https://huggingface.co/THUDM/visualglm-6b
~~~

VisualGLM-6B 由 SwissArmyTransformer(简称sat) 库训练，这是一个支持Transformer灵活修改、训练的工具库，支持Lora、P-tuning等参数高效微调方法，所以如果不成功的话，建议下载SwissArmyTransformer源码手动安装。如果一切顺利执行以下就可以运行可视化服务。显存大小约占16G左右。

~~~PYTHON
pip install SwissArmyTransformer
~~~

~~~PYTHON
python web_demo_hf.py
~~~

## 模型推理

基于Huggingface的transformers库调用模型：

~~~python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/visualglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/visualglm-6b", trust_remote_code=True).half().cuda()
image_path = "your image path"
response, history = model.chat(tokenizer, image_path, "描述这张图片。", history=[])
print(response)
response, history = model.chat(tokenizer, "这张图片可能是在什么场所拍摄的？", history=history)
print(response)
~~~

基于SwissArmyTransformer库调用模型：

~~~python
import argparse
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)

#----------------------------------------------------
from model import chat, VisualGLMModel
model, model_args = VisualGLMModel.from_pretrained('visualglm-6b', args=argparse.Namespace(fp16=True, skip_init=True))

#----------------------------------------------------
from sat.model.mixins import CachedAutoregressiveMixin
model.add_mixin('auto-regressive', CachedAutoregressiveMixin())
image_path = "your image path or URL"
response, history, cache_image = chat(image_path, model, tokenizer, "描述这张图片。", history=[])
print(response)
response, history, cache_image = chat(None, model, tokenizer, "这张图片可能是在什么场所拍摄的？", history=history, image=cache_image)
print(response)
~~~

命令行执行Demo

~~~shell
python cli_demo.py 
or
python cli_demo_hf.py
~~~

网页版Demo

~~~python
git clone https://github.com/THUDM/VisualGLM-6B
cd VisualGLM-6B
python web_demo.py
or
python web_demo_hf.py
~~~
